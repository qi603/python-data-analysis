{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pandas\n",
    "\n",
    "<img src=\"img/pandas_logo.png\" height=60% width=60%>\n",
    "\n",
    "Content:\n",
    "- 2.1 Introduction\n",
    "- 2.2 Installation\n",
    "- 2.3 Series\n",
    "- 2.4 DataFrames\n",
    "- 2.5 Import files\n",
    "- 2.6 Summary\n",
    "\n",
    "## 2.1 Introduction\n",
    "\n",
    "Pandas is a Python library that allows us to easily manipulate data. It is considered the de-facto standard to read, analyze and visualize tabular data from CSV files, Excel tables, SQL tables and many more.\n",
    "\n",
    "Pandas has three main data structures: \n",
    "- Series: a 1D array\n",
    "- DataFrame: a 2D table\n",
    "- Panel: a 3D array (not discussed here)\n",
    "\n",
    "Although the main data structure for tables is called a DataFrame, it is important to understand that it is built as a combination of Series. Generally, however, we will talk about the index (rows) and the columns.  \n",
    "\n",
    "<img src=\"img/pandas-df.png\" height=70% width=70%>\n",
    "\n",
    "There are a lot of comparisons that can be drawn between pandas and the R language. So if you have experience in R [this website](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html) might give you an interesting comparison between both.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Installation\n",
    "\n",
    "There are a couple of ways to install pandas (or any other library). \n",
    "- In the Environments section of Anaconda Navigator and manually selecting the package, \n",
    "- Installing in the Conda environment: `conda install -c conda-forge pandas`, \n",
    "- Installing in the Jupyter Notebook: `pip install pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will result in an effective installation, or might tell you that the requirements are already satisfied. You might want to refresh your Notebook for the changes to take place. \n",
    "\n",
    "After installing the library, we still have to import it in our Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas as pd is a convention\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Series\n",
    "\n",
    "We will start by creating a one dimensional array which is the equivalent of a pandas Series. The data that we gather in this array is the data that will make up for a column in our table later on. \n",
    "- The data can be of any kind: integers, strings, floats, Python objects, etc. \n",
    "- The data is always accompanied by an index\n",
    "\n",
    "<img src=\"img/series.PNG\" width=\"300\" height=\"200\"/>  \n",
    "\n",
    "Source: [berbagaidata.blogspot.com](https://berbagaidata.blogspot.com/2019/05/python-for-data-analysis-data-wrangling_17.html)\n",
    "\n",
    "\n",
    "There are several ways to **create a Series**: from a list, a dictionary, a NumPy array and from files and other external data sources. Let's make our first pandas object starting from an ordinary Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas Series from list\n",
    "counts = [12, 35, 45, 12, 22, 38]\n",
    "\n",
    "countSeries = pd.Series(counts)\n",
    "type(countSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas Series from tuple\n",
    "genes = ('GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF')\n",
    "\n",
    "genesSeries = pd.Series(genes)\n",
    "genesSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we already have a table, however the first column is actually the index and the second column contains the data. If the **index** is not specified upon making the pandas Series, the default will be a scalar value ranging from 0 uptill the length of the array -1. \n",
    "\n",
    "*`range(n) where n is array length, i.e., [0,1,2,3â€¦. len(array)-1].`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code adds an index as an argument. It makes a Series where \n",
    "- the list of genes will be the index of the object, and \n",
    "- the list of counts will be the actual data in a column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySerie1 = pd.Series(counts, index=genes)\n",
    "mySerie1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series can also be made from a **dictionary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas Series from dictionary\n",
    "aaDict = {\n",
    "     'A': 'Ala',\n",
    "     'C': 'Cys',\n",
    "     'D': 'Asp',\n",
    "     'E': 'Glu',\n",
    "     'F': 'Phe',\n",
    "     'G': 'Gly',\n",
    "     'H': 'His',\n",
    "     'I': 'Ile',\n",
    "     'K': 'Lys',\n",
    "     'L': 'Leu',\n",
    "     'M': 'Met',\n",
    "     'N': 'Asn',\n",
    "     'P': 'Pro',\n",
    "     'Q': 'Gln',\n",
    "     'R': 'Arg',\n",
    "     'S': 'Ser',\n",
    "     'T': 'Thr',\n",
    "     'V': 'Val',\n",
    "     'W': 'Trp',\n",
    "     'Y': 'Tyr'}\n",
    "\n",
    "aaSeries = pd.Series(aaDict)\n",
    "aaSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the keys of the dictionary will be the indeces in the Series and the respective values will be the data values associated with these indeces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously mathematical operations can be performed on the complete dataset in a Series. These are called **vectorized operations** as the operation will be applied to each element in the Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized operations \n",
    "countSeries + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized operations \n",
    "countSeries * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to apply functions on a pandas Series. Here are two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pandas functions \n",
    "countSeries.mean()\n",
    "countSeries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Python built-in function can you use to list all functions applicable on pandas Series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.3.1 \n",
    "\n",
    "Use one of pandas Series functions to add the following data to the `mySerie1` pandas Series object as a new row:\n",
    "\n",
    "```\n",
    "GeneK   25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally retrieving an element is easily done with squared brackets, similarly to accessing elements from a data structures in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a single element in pandas Series\n",
    "mySerie1['GeneA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing multiple values in a pandas Series object\n",
    "mySerie1[['GeneA', 'GeneC', 'GeneF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing multiple values in a pandas Series object\n",
    "mySerie1[:2] # 0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 DataFrame\n",
    "The pandas DataFrame is a two-dimensional data structure, essentially it's a combination of two or more Series objects: \n",
    "\n",
    "\n",
    "<img src=\"img/series-and-dataframe.PNG\" width=\"550\" height=\"200\"/>  \n",
    "\n",
    "Source: [berbagaidata.blogspot.com](https://berbagaidata.blogspot.com/2019/05/python-for-data-analysis-data-wrangling_17.html)\n",
    "\n",
    "\n",
    "\n",
    "DataFrames consist of row indeces and column indeces with the data in the columns as visually depicted in the following table.  \n",
    "\n",
    "|  /   | **column_index1** | **column_index2**   |\n",
    "|---|---|---|\n",
    "| **row_index1**   | 1  | 2  |   \n",
    "| **row_index2**  |  3 | 4  |   \n",
    "| **row_index3**  |  5 | 6  |   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Series, **a DataFrame can be created in several ways**: from lists, dictionaries, or Series. We will cover several possibilities here below, however, the most important one is probably from importing a dataset from a file (section 2.5). \n",
    "\n",
    "First we'll create a DataFrame from a **dictionary** where the key is the column index and the value is a list of values that represent the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "help(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "help(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_exp1 = [12, 35, 45, 12, 22, 38]\n",
    "counts_exp2 = [6, 28, 55, 12, 19, 34]\n",
    "genes = ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF']\n",
    "\n",
    "# Dictionary with key = column index, and values = data values\n",
    "dataframe_dict = {'counts_exp1': counts_exp1, 'counts_exp2': counts_exp2}\n",
    "\n",
    "# Dictionary with values and list of indeces\n",
    "df = pd.DataFrame(dataframe_dict, index = genes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **inspect** the DataFrame we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.index)\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First three rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last five rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exploring how to access rows, columns and elements from a table, let's have a look at how we can **add new data or delete a column** from a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new column with new data, similar to adding data to dictionaries\n",
    "df['counts_exp3'] = [23, 24, 58, 16, 8, 5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, using the insert method\n",
    "df.insert(loc = 1, column = \"counts_exp4\", value = [3, 4, 35, 16, 42, 11], allow_duplicates = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting a column in two ways\n",
    "# del df['counts_exp4']\n",
    "# or\n",
    "df.drop('counts_exp4', axis = 1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new row data\n",
    "dict_row = {'counts_exp1': 1, 'counts_exp2': 2, 'counts_exp3': 3} \n",
    "\n",
    "# Will ruin the row indeces:\n",
    "#df = df.append(df_row, ignore_index = True) \n",
    "\n",
    "# First create Series\n",
    "new_row = pd.Series(data = dict_row, name='GeneX')\n",
    "# Append row to the dataframe, ignore index is False as we want to keep our indeces\n",
    "df = df.append(new_row, ignore_index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark: iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accessing** data is done by calling the index name of the column within squared brackets. \n",
    "Remember that Python starts counting from 0 and it excludes the last number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the column counts_exp1\n",
    "df['counts_exp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing multiple columns \n",
    "df[['counts_exp1', 'counts_exp2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing values within a column: rows from 2 to 3. \n",
    "#df['counts_exp1'][1:3]\n",
    "\n",
    "# Which is the same as:\n",
    "df[1:3]['counts_exp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing all columns from rows 2 to 4\n",
    "df[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of accessing the data in a Dataframe is by using the `.loc[]` and `iloc[]` method. \n",
    "- `.loc[]`: uses primarily label(s) to access the data,\n",
    "- `.iloc[]`: uses purely integer-location based indexing for selection by position.\n",
    "\n",
    "**`.loc[]`** accepts the row index as a first and default parameter, if a second parameter is given, this should refer to the column index. An elaborate explanation of the possibilities with the location method is available [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one row with single label\n",
    "df.loc['GeneE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple rows with list of labels\n",
    "df.loc[['GeneD', 'GeneE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple rows with slice object\n",
    "df.loc['GeneC':'GeneE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['GeneC', 'counts_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing multiple rows from a dataset\n",
    "selected_genes = ['GeneA', 'GeneB' ,'GeneC', 'GeneF']\n",
    "df.loc[selected_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing multiple rows and a subselection of columns by passing two parameters to .loc[]\n",
    "selected_counts = ['counts_exp1', 'counts_exp2']\n",
    "df.loc[selected_genes, selected_counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of slicing our dataset is by using `.iloc[]`. An elaborate explanation of the possibilities with the location method is available [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html). Try the following, can you figure out how this method works? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[0]\n",
    "df.iloc[[0]]\n",
    "df.iloc[[1,2,3],[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final suggestion, the [Pandas Cheat Sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf) can be very helpful for remembering `pandas` operations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.4.1\n",
    "Start with the dataframe defined below and:\n",
    "- Select the number of counts in *GeneD* for the second and third experiment. \n",
    "- Add a new column to the dataframe with the average of the three experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "counts_exp1 = [12, 35, 45, 12, 22, 38]\n",
    "counts_exp2 = [6, 28, 55, 12, 19, 34]\n",
    "counts_exp3 = [23, 24, 58, 16, 8, 5]\n",
    "genes = ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF']\n",
    "\n",
    "# Dictionary with key = column index, and values = data values\n",
    "dataframe_dict = {'counts_exp1': counts_exp1, 'counts_exp2': counts_exp2, 'counts_exp3': counts_exp3}\n",
    "\n",
    "# Dictionary with values and list of indeces\n",
    "df = pd.DataFrame(dataframe_dict, index = genes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.4.2\n",
    "Start with the same dataframe as we used earlier.\n",
    "- Search in the pandas documentation for the median method and add a column that describes the median countvalues per gene.\n",
    "- Search in the pandas documentation for a method that will count all of the values of one experiment and add it as an extra row to the table. \n",
    "- Remove the row with the sum of the counts that we added in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 File I/O\n",
    "Now we know how to parse through our Dataframe tables and manipulate the data, we'll have a look at how to import data from files. Pandas is great for working with tabular data, hence generally the data will come from a spreadsheet (csv, excel, tab-separated txt files, etc.). There are plenty of possibilities hence we refer to the [pandas documentation regarding file I/O](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html). \n",
    "\n",
    "Importing data is done by referring to a local file explicitly. This file can be in your local folder, or can be imported from the internet. \n",
    "\n",
    "The following file contains data of an experiment that detects occurrences of methylated cytosines. After some data analysis we obtain the following table with - per chromosome - the distribution of methylated cytosines in different regions of the gene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metagenic classification file in the data folder - csv\n",
    "metagenic_csv = pd.read_csv('data/metagenic.csv')\n",
    "metagenic_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of all the parameters that `.read_csv` accepts, is accessible on the pandas documentation website ([here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.5.1 - A\n",
    "Search for the parameters of `.read_csv` that you need in order to read in the `metagenic.csv` file where:\n",
    "- chromosomes are the index of the rows, and \n",
    "- only the first 10 rows are imported.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2.5.1 - B\n",
    "Import the data from the `metagenic.csv` file and add a new column with the total counts for each chromosome (e.g. chromosome 21 has 88 counts), and sort the table by descending total counts per chromosome.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block we will read in the more widely known, iris dataset. In this case, we're downloading and importing it immediately from a GitHub repository accessible via the given link below. In the next chapter we will see that it is part of the Seaborn visualization library though.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea on the descriptive statistics and the shape of the dataset's distribution, we can use `.describe()`. This method can be applied on the table (numerical data only) or on a selection of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris['sepal_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in this dataset we're facing different kinds of data:\n",
    "- numerical data (sepal_length, sepal_width, petal_length and petal_width)\n",
    "- categorical data (species)\n",
    "\n",
    "and therefore need to be treated differently. Luckily, pandas allows us to do this in a very easy way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect datatypes of the columns:\n",
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the species column to categorical data explicitly\n",
    "iris['species'] = iris['species'].astype('category')\n",
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of the categorical data\n",
    "iris['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values within a column\n",
    "iris['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the counted occurrences of the categorical data\n",
    "iris['species'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping data & use mathematical operation to summarize the data\n",
    "iris.groupby(['species']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "iris.groupby(by='species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract the row/data from versicolor species. \n",
    "iris.loc[iris['species'] == 'versicolor']\n",
    "\n",
    "#iris.loc[iris['species'] == 'versicolor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.5.2 \n",
    "Can you find a method that will retrieve the indices of all the virginica flowers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.5.3\n",
    "From the file `metagenic.csv`:\n",
    "1. Sort the table based on the counts in exons in descending way  \n",
    "2. Make a subselection of chromosomes with at least 15 counts in introns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2.5.4\n",
    "\n",
    "For this exercise we will use [this dataset](https://datahub.io/core/pharmaceutical-drug-spending) which contains the spendings of a bunch of countries in the pharmaceutical industry as from 1971. The dataset is available in the data folder as `pharmaspending.csv`. \n",
    "\n",
    "Make a subselection of this dataset that contains the data for Belgium and its neigbhouring countries France, Germany and the Netherlands. Furthermore, we're only interested in the data starting from the year 2000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2.5.5 \n",
    "In this exercise, derived from the [GTN](https://galaxyproject.github.io/training-material/topics/transcriptomics/tutorials/rna-seq-viz-with-heatmap2/tutorial.html), we will prepare the data to create a heatmap (see exercise 3.2.6) of the top differentially expressed genes in an RNA-seq counts dataset. \n",
    "- [`counts`](https://zenodo.org/record/2529926/files/limma-voom_normalised_counts)\n",
    "- [`de_genes`](https://zenodo.org/record/2529926/files/limma-voom_luminalpregnant-luminallactate)  \n",
    "\n",
    "The latter file contains the results from comparing gene expression in the luminal cells in the pregnant versus lactating mice. It includes genes that are not significantly differentially expressed. Weâ€™ll call genes significantly differentially expressed in this dataset if they pass the thresholds of `P-value < 0.01` and `fold change of > 1.5 (log2FC of 0.58)`. Filter the top 20 DE genes from that table and create a joint dataframe that contains only the following columns and looks like this:\n",
    "\n",
    "| SYMBOL_x |  MCL1.DG |  MCL1.DH |  MCL1.DI |  MCL1.DJ |   MCL1.DK |   MCL1.DL |  MCL1.LA |  MCL1.LB |  MCL1.LC |  MCL1.LD |  MCL1.LE |  MCL1.LF |\n",
    "|---------:|---------:|---------:|---------:|---------:|----------:|----------:|---------:|---------:|---------:|---------:|---------:|----------|\n",
    "|     Ggt1 | 6.732347 | 6.556047 | 6.558849 | 6.586562 |  6.437596 |  6.394067 | 5.193118 | 5.526432 | 4.223990 | 4.341605 | 7.243899 | 7.354535 |\n",
    "|  Slc39a4 | 2.722153 | 3.027691 | 2.175532 | 1.993214 | -0.193255 | -0.016902 | 3.071502 | 2.928202 | 6.472918 | 6.526836 | 2.430346 | 1.847241 |\n",
    "|      Ppl | 5.102274 | 4.900942 | 5.755087 | 5.951023 |  6.851420 |  6.881858 | 7.359977 | 7.732010 | 8.227118 | 8.437499 | 4.646145 | 4.798986 |\n",
    "| ...   |     ...     |      ...    |   ...       |        ...  |    ...       |         ...  |       ...  |        ...   |      ...    |      ...    |       ...   |       ...   |\n",
    "\n",
    "Save the file as a csv-file in the data-folder. You can use the lay-out given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "# Set filters\n",
    "p_adj_lim = 0.01\n",
    "logFC_lim = 0.58\n",
    "\n",
    "# Filter the non-significantly differentially expressed genes out\n",
    "\n",
    "# Sort the remaining significantly expressed genes (highest DE genes on top)\n",
    "\n",
    "# Filter the top 20 DE genes\n",
    "\n",
    "# Create dataframe for heatmap that is a joined dataframe of the two imported data files\n",
    "\n",
    "# Make a subselection of the columns (genes and DE, see the df above)\n",
    "\n",
    "# Set the names of the genes as the row index\n",
    "\n",
    "# Store the dataframe in a csv file for later usage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Summary\n",
    "We've seen a bunch of methods on how we can read and manipulate data with pandas. As it is such a huge library, we could still spend tons of time discovering all the modules, however now you should be able to come up with solutions tailored to a specific question. Either by using the pandas documentation (recommended) or by searching on Stackoverflow or any other forum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Next session\n",
    "Explore how to visualize your data in the [next chapter](03_Visualization.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
